{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Statistics for Quantitative Trading\n",
    "<div class=\"alert alert-info\"><strong>Part II : Time Series Modeling with Python</strong></div>\n",
    "\n",
    "#### Notebook Created on: 6 July 2020\n",
    "##### Last Update: 6 Jan 2023\n",
    "##### Version 4.5\n",
    "##### Author: Vivek Krishnamoorthy\n",
    "\n",
    "## **Agenda for today**\n",
    "\n",
    "- Modeling time series using decomposition\n",
    "    - A fleeting exploration of oil price seasonality\n",
    "    - Using the `statsmodels` library for decomposition\n",
    "- A closer look at ACF and PACF\n",
    "- Testing for stationarity\n",
    "- Modeling time series using\n",
    "    - AR\n",
    "    - MA\n",
    "    - ARIMA\n",
    "- Modeling and forecasting volatility using ARCH/GARCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'convert'></a>\n",
    "\n",
    "### Anatomy of a time series process\n",
    "\n",
    "Time series processes have a wide variety of patterns. So it is helpful to consider them as a combination of *systematic* and *unsystematic* components.\n",
    "\n",
    "- **Systematic**: These are recurring in nature and so can be described and modeled.\n",
    "- **Non-systematic**: These are random in nature and so cannot be directly modeled.\n",
    "\n",
    "The systematic components can be further split into *level*, *trend*, and *seasonality* whereas the non-systematic component is referred to as *noise*.\n",
    "\n",
    "- **Level**: The average value of the process.\n",
    "- **Trend**: The direction and rate of change of the process. The slope is a good proxy for it.\n",
    "- **Seasonality**: Deviations in the process caused by recurring short-term cycles.\n",
    "- **Noise**: The random variation observed in the process.\n",
    "\n",
    "Another useful abstraction while analyzing time series processes is to see them as either an *additive* or a *multiplicative* blend of the four constituent parts mentioned.\n",
    "\n",
    "**Additive model**: The process $X(t)$ has the form\n",
    "$$X(t) = Level + Trend + Seasonality + Noise$$\n",
    "\n",
    "We use an additive model when the underlying process under examination has the following characteristics.\n",
    "- The process changes remain constant over time (i.e. they are linear). So the trend line would be straight.\n",
    "- It shows linear seasonality. That is to say the frequency and amplitude (i.e. the width and the height) of the cycles remain constant over time.\n",
    "\n",
    "\n",
    "\n",
    "**Multiplicative model**: The process $X(t)$ has the form\n",
    "$$X(t) = Level \\times Trend \\times Seasonality \\times Noise$$\n",
    "\n",
    "We use a multiplicative model when the underlying process under examination has the following characteristics.\n",
    "- The process changes vary over time (i.e. they are non-linear in nature).\n",
    "- An exponential or quadratic or higher order polynomial process is multiplicative. So the trend-line would be curved, not straight. \n",
    "- It shows non-linear seasonality. That is to say the frequency and amplitude (i.e. the width and the height) of the cycles vary over time.\n",
    "\n",
    "In the (harsh) real world, we often encounter non-linear or even mixed processes and therefore have to work with the multiplicative model as our prototype. But we prefer to work with linear processes as they are easier to use. So we transform the original process into a linear one. A commonly used trick is applying a log transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling a time series using decomposition\n",
    "\n",
    "There are several available libraries (ex. [`statsmodels`](https://www.statsmodels.org/stable/tsa.html), [`prophet`](https://facebook.github.io/prophet/docs/quick_start.html), [`scikit-learn`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html), [`PyFlux`](https://pyflux.readthedocs.io/en/latest/), [`fecon236`](https://github.com/MathSci/fecon236), [`PM-Prophet`](https://github.com/luke14free/pm-prophet) at the time of this writing) in Python to develop sophisticated time series models for forecasting. \n",
    "\n",
    "We work with `statsmodels`. Both offer convenient routines to automatically decompose a time series into their fundamental components.\n",
    "\n",
    "To illustrate the ideas, we make use of the daily historical prices of crude oil (from 2003 to 2022) and soybean (from 2000 to 2020).\n",
    "\n",
    "> *The OPEC Reference Basket (ORB), also referred to as the OPEC (Organization of Petroleum Exporting Countries ex. Qatar, Saudi Arabia, Iran, Iraq) Basket, is a weighted average of prices for petroleum blends produced by OPEC members. It is used as an important benchmark for crude oil prices. The OPEC Basket, including a mix of light and heavy crude oil products, is heavier than both Brent crude oil, and West Texas Intermediate crude oil.* - [Source](https://www.investopedia.com/terms/o/opecbasket.asp)\n",
    "\n",
    "> *The soya bean is a species of legume and is one of the worldâ€™s most important oil plants, since around half of vegetable oil produced is obtained from the soya bean. It is particularly significant because of its protein content (39 per cent) and its oil content (17 per cent), since no other plant offers comparable values. Soya is also used as an ingredient and additive in the food industry. It is estimated that around 30,000 foods contain ingredients derived from soya. Soya milk can be produced from ground yellow soya beans, and then can be processed into tofu. Tofu is used as a meat substitute in vegetarian cookery because of its high protein content and because it contains all the essential amino acids.* - [Source](https://markets.businessinsider.com/commodities/soybeans-price/usd)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We try the `seasonal_decompose` method from the `statsmodels.tsa` sub-library.\n",
    "\n",
    "### Seasonal decomposition using the `seasonal_decompose` routine in `statsmodels` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "import nasdaqdatalink\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end4 = datetime.date(2022, 12, 31)\n",
    "start4 = datetime.date(2003, 1, 1)\n",
    "\n",
    "ticker4 = \"OPEC/ORB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ***********************************************************\n",
    "## ***** EXPERIMENTAL : IGNORE THIS CELL *********************\n",
    "## Trials with other commodity data\n",
    "## ***********************************************************\n",
    "\n",
    "# \"OPEC/ORB\" this is crude oil prices per barrel\n",
    "# WGC/GOLD_DAILY_INR for daily gold prices in India available until 10 March 2020\n",
    "# TFGRAIN/SOYBEANS for daily soy bean prices per bushel\n",
    "# WORLDAL/PALPROD primary aluminium production across continents. not tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = nasdaqdatalink.get(dataset=ticker4, start_date=start4, end_date=end4)\n",
    "print(f\"Downloaded {df.shape[0]} rows and {df.shape[1]} columns of {ticker4} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df4.head(10))\n",
    "print(df4.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.rename(columns={'Value': 'price'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_long_run_mean = np.round(df4['price'].mean(), 1)\n",
    "print(oil_long_run_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.plot(figsize=(12, 9), linewidth=0.8, color='indigo')\n",
    "plt.title(f\"{ticker4} prices over time\", fontsize=20)\n",
    "for eachyear in range(start4.year, end4.year):\n",
    "    plt.axvline(pd.to_datetime(str(eachyear)+'-12-31'), color='black', linestyle='--', alpha=0.4)\n",
    "\n",
    "plt.axhline(df4['price'].mean(), label='Mean price', color='maroon', linestyle='--', alpha=0.7)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- There are upward and downward trends in the prices. Looks linear. Needs further probing.\n",
    "- There seems to be seasonality and we can investigate further by looking at some moving averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = 21 # for 1 monthly moving average calculations\n",
    "window_length2 = 252 # for annual moving average calculations\n",
    "\n",
    "## Calculating 21-day rolling mean and volatility\n",
    "\n",
    "df4['rolling_21d_mean'] = df4['price'].rolling(window=window_length).mean()\n",
    "df4['rolling_21d_vol'] = df4['price'].rolling(window=window_length).std()\n",
    "\n",
    "\n",
    "## Calculating 252-day rolling mean and volatility\n",
    "\n",
    "df4['rolling_12m_mean'] = df4['price'].rolling(window=window_length2).mean()\n",
    "df4['rolling_12m_vol'] = df4['price'].rolling(window=window_length2).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.25)\n",
    "df4.plot(figsize=(12, 9), linewidth=0.8)\n",
    "\n",
    "for eachyear in range(start4.year, end4.year):\n",
    "    plt.axvline(pd.to_datetime(str(eachyear)+'-12-31'), color='black', linestyle='--', alpha=0.4)\n",
    "\n",
    "\n",
    "plt.title(\"OPEC benchmark crude oil prices over time\", fontsize=20)\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Price per barrel (in US$)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- The yearly moving average of the prices show a linear trend (which changes roughly every couple of years).\n",
    "- The monthly moving price average shows seasonality.\n",
    "- The rolling volatility is time-varying in both (monthly and annual) cases.\n",
    "- Let's try using the **multiplicative** model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Enter rabbit hole: A fleeting exploration of oil price seasonality_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4x = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4x = df4x.resample(\"M\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4x.rename(columns={'Value': 'monthly_mean_price'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4x['year'] = df4x.index.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4x['month'] = df4x.index.month_name().str[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4x.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define figure size with width = 16 and height = 10\n",
    "\n",
    "sns.set(rc={\"figure.figsize\":(16, 10)})\n",
    "\n",
    "# box plot grouping by month to check for trend changes monthly\n",
    "\n",
    "df4xbox = sns.boxplot(x='month', y='monthly_mean_price', data=df4x, \n",
    "                   order=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], \n",
    "                   palette='Set2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "- There appears to be an increasing trend in mean price per barrel from January to July followed by a decrease in mean price from August to December.\n",
    "- The inter quantile range (IQR) is the lowest in the last three months of the year and coincides with the holiday season in many parts of the world.\n",
    "\n",
    "\n",
    "### _Exit rabbit hole: A fleeting exploration of oil price seasonality_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuing with using `statsmodels`\n",
    "\n",
    "I now use the multiplicative decomposition model to extract seasonality observed in the ORB prices. I use the `seasonal_decompose` routine with `period=252`. That is to say, the trend repeats every 252 days (# of trading days in each calendar year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decompose_series = seasonal_decompose(df4['price'], period=252, model=\"multiplicative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 1, figsize=(15, 10))\n",
    "\n",
    "decompose_series.observed.plot(ax=ax[0])\n",
    "ax[0].set_title(\"Time series of crude oil prices\", fontsize=16)\n",
    "ax[0].set(xlabel=\"\", ylabel=\"Oil price (in US$/barrel)\")\n",
    "\n",
    "\n",
    "decompose_series.trend.plot(ax=ax[1])\n",
    "ax[1].set(xlabel=\"\", ylabel=\"Trend\")\n",
    "\n",
    "\n",
    "decompose_series.seasonal.plot(ax=ax[2])\n",
    "ax[2].set(xlabel=\"\", ylabel=\"Seasonal\")\n",
    "\n",
    "\n",
    "decompose_series.resid.plot(ax=ax[3])\n",
    "ax[3].set(xlabel=\"Date\", ylabel=\"Residual\")\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decompose_series.seasonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "- The plot shows us the observed, trend, seasonal and residual time series. We can access each component by typing: `decompose_series.trend`, `decompose_series.seasonal`, and `decompose_series.residual` \n",
    "- The trend plot shows an increasing trend until 2008 and then a decreasing trend until 2009, followed by some ups, plateaus and downs until 2022.\n",
    "- The seasonal plots shows a repeating trend every year.\n",
    "- The residual plot clearly has non-constant volatility. If the model was a suitable fit, then after taking out the trend and seasonality present in the price data, we would have residuals that do not have any discernable pattern. Not so here.\n",
    "- At this stage, we would evaluate alternatives to model the residuals. We could even consider exogenous variables like oil production, renewable energy investments, etc. which would influence oil prices (outside the scope of this session).\n",
    "- From the `statsmodels` documentation: *This is a naive decomposition. More sophisticated methods should be preferred.*\n",
    "\n",
    "Let's now inspect a two year period (I pick the years 2015 and 2016) closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_price_seasonality = decompose_series.seasonal[df4.index.year.isin([2015, 2016])]\n",
    "ma_oil_price = oil_price_seasonality.rolling(window=21).mean()\n",
    "plt.plot(oil_price_seasonality, label='ORB seasonal movements')\n",
    "plt.plot(ma_oil_price, label='21-day MA')\n",
    "plt.title(\"ORB price seasonality\", fontsize=18, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A closer look at ACF and PACF\n",
    "\n",
    "Consider a supermarket near your home which has deep discounts on staples (rice, flour, sugar, potatoes) on Tuesdays, Thursdays and Saturdays each week.\n",
    "The price of staples on Monday will directly influence their prices on Wednesday, the prices on Tuesday will directly affect their prices on Thursday and so on.\n",
    "\n",
    "So we can calculate this influence (of 2-day old prices and the present day's prices) using the well known idea of correlation. How?\n",
    "\n",
    "We line up 2-day old prices in a column and the present day's prices in another column. \n",
    "We then compute the correlations of these two columns of numbers. That's it!\n",
    "\n",
    "We call this auto-correlation or serial correlation. 'Auto' here means 'self'. Since we're computing the correlation of a variable with itself (i.e. a past version of itself).\n",
    "\n",
    "Mathematically, we can write this as $Corr(S_t, S_{t-2})$.\\\n",
    "This is also called the ACF at lag 2.\n",
    "\n",
    "As we see, calculating the autocorrelation is quite straightforward. \n",
    "It's Pearson's correlation.\n",
    "\n",
    "When we look at the influences on say, Wednesday's prices, there are mainly two effects in operation\n",
    "- Monday prices will influence Tuesday prices which influence Wednesday's prices. i.e. the indirect effect of Monday's prices via Tuesday's prices are seen on Wednesday's prices.\n",
    "- Monday's prices directly influence Wednesday's prices\n",
    "\n",
    "![Image](acf_pacf_2.jpg)\n",
    "\n",
    "The PACF helps us in capturing ONLY the direct effect of Monday's prices on Wednesday's prices.\\\n",
    "It does so via multiple regression where $S_t = \\phi_{21}S_{t-1} + \\phi_{22}S_{t-2} + \\epsilon_t$\\\n",
    "Here $\\phi_{22}$ will be the PACF at lag 2.\n",
    "\n",
    "Fortunately, we don't need to delve into the thorny math and modeling to compute PACF or ACF at different lags. There are many statistical libraries like `statsmodels` which do it with a single line of code.\n",
    "\n",
    "#### In summary,\n",
    "\n",
    "- The PACF at lag $k$, is the **direct** correlation between the value of something today and its value $k$ periods back i.e. it removes the indirect effects that occur via shorter lags $k-1, k-2, k-3,..., 1$\n",
    "- The ACF at lag $k$, is the correlation **(direct + indirect)** between the value of something today and its value $k$ periods back.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for stationarity\n",
    "\n",
    "There are three ways of checking for stationarity in a time series.\n",
    "1. Visual inspection\n",
    "2. Statistical tests\n",
    "3. ACF/PACF plots\n",
    "\n",
    "We prefer working with stationary time series processes because it makes modeling, analysis and forecasting more feasible.\n",
    "\n",
    "For this section, we work with the last 25 years of daily gold prices in India. The prices shown are denominated in INR per ounce.\n",
    "\n",
    "> *Gold is a liquid asset, ranking at levels comparable to many global stock markets as well as currency spreads. Its liquidity is often sourced during periods of stress in the markets, one of its appealing qualities.* - [Source](https://www.gold.org/goldhub/data/trading-volumes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start6 = '1995-01-01'\n",
    "end6 = '2021-09-29'\n",
    "ticker6 = \"WGC/GOLD_DAILY_INR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = nasdaqdatalink.get(dataset=ticker6, start_date=start6, end_date=end6)\n",
    "print(f\"Downloaded {df.shape[0]} rows and {df.shape[1]} columns of {ticker6} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "################# PLEASE USE BELOW STATEMENTS IF NEEDED #####################\n",
    "#############################################################################\n",
    "\n",
    "## If you have don't have quandl, you can read the csv file as shown.\n",
    "\n",
    "# mydateparser = lambda x: pd.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S%z\")\n",
    "# df.to_csv(\"gold_prices_inr.csv\")\n",
    "# df4 = pd.read_csv(\"gold_prices_inr.csv\", index_col=0, parse_dates=True)\n",
    "# df1 = pd.read_csv(\"NSE_5min_interval.csv\", index_col=0, parse_dates=True, date_parser=mydateparser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.rename(columns={\"Value\": \"price\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats as sms\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import acf, pacf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stationarity(y, wl1=21, wl2=252, lags=40, figsize=(18, 12)):\n",
    "    \"\"\" Checks the stationarity of a pandas Series (default is daily prices or returns),\n",
    "        using plots, correlograms and the ADF test\n",
    "    \"\"\"\n",
    "    ## Calculating rolling statistics\n",
    "    \n",
    "    rolling_wl1_mean = y.rolling(window=wl1).mean()\n",
    "    rolling_wl2_mean = y.rolling(window=wl2).mean()\n",
    "    rolling_wl1_vol = y.rolling(window=wl1).std()\n",
    "    rolling_wl2_vol = y.rolling(window=wl2).std()\n",
    "    \n",
    "    ## Plotting the price, rolling statistics and correlograms\n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 14))\n",
    "    sns.set(font_scale=1)\n",
    "    layout = (2, 2)\n",
    "    y_ax = plt.subplot2grid(layout, (0, 0))\n",
    "    vol_ax = plt.subplot2grid(layout, (0, 1))\n",
    "    acf_ax = plt.subplot2grid(layout, (1, 0))\n",
    "    pacf_ax = plt.subplot2grid(layout, (1, 1))\n",
    "        \n",
    "    y.plot(ax=y_ax)\n",
    "    rolling_wl1_mean.plot(ax=y_ax)\n",
    "    rolling_wl2_mean.plot(ax=y_ax)\n",
    "    \n",
    "    rolling_wl1_vol.plot(ax=vol_ax)\n",
    "    rolling_wl2_vol.plot(ax=vol_ax)\n",
    "    y_ax.set_title('Rolling means over time')\n",
    "    y_ax.legend(['observed', f'{wl1}-period MA of observed', f'{wl2}-period MA of observed'], loc='best')\n",
    "    #y_ax.set_ylabel(\"Gold prices(in INR)/oz.\")\n",
    "    \n",
    "    vol_ax.set_title('Rolling volatility over time')\n",
    "    vol_ax.legend([f'{wl1}-period MA of volatility', f'{wl2}-period MA of volatility'], loc='best')\n",
    "    \n",
    "    sm.graphics.tsa.plot_acf(y, lags=lags, ax=acf_ax, alpha=0.05)\n",
    "    sm.graphics.tsa.plot_pacf(y, lags=lags, ax=pacf_ax, alpha=0.05)\n",
    "    \n",
    "    ## Running the Augmented Dickey-Fuller test\n",
    "    print('--------------------------------------------------------------')\n",
    "    print('--------- The augmented Dickey-Fuller test results -----------')\n",
    "    print('--------------------------------------------------------------')\n",
    "    adftest = adfuller(y, autolag='AIC')\n",
    "    results = pd.Series(adftest[0:4], index=['Test Statistic','p-value','# of Lags','# of Observations'])\n",
    "    for key,value in adftest[4].items():\n",
    "        results[f'Critical Value ({key})'] = '{0:.3f}'.format(value)\n",
    "    print(results)\n",
    "    print('--------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_stationarity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_stationarity(df6['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- In the ADF test, if the test statistic is greater than the critical value, we conclude that the series is non-stationary. We can draw the same conclusion by examining the p-value. A p-value greater than our significance level (conventionally 5%) means we cannot reject our null hypothesis (The series is not stationary). \n",
    "- For the gold prices, we have a p-value of nearly 1 (and equivalently the test statistic is greater than the critical values at all 3 significance levels), so we conclude that the price series is not stationary.\n",
    "- The rolling means and volatility plots are time-varying. So we also conclude visually that gold prices in India are non-stationary.\n",
    "- From the ACF, there are significant autocorrelations above the 95% confidence interval at all lags. From the PACF, we have significance in autocorrelations at lags 1, 2, 3, 6, and 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6['log_returns'] = np.log(df6['price'] / df6['price'].shift(1))\n",
    "df6.dropna(axis='rows', how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_stationarity(df6['log_returns'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- As per the ADF test results, the returns of gold are stationary since the p-value is almost 0 and the test statistic is less than all the critical values.\n",
    "- The returns and rolling means of the returns are all centred around 0. As the time scale increases, the means become more and more constant. At shorter time scales, the noise tends to obscure the signal.\n",
    "- The volatily is time-varying at both the faster and slower rolling levels.\n",
    "- There are little spikes in the ACF plot at lags 3, 11, and 21."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method III : Modeling time series using ARIMA models\n",
    "\n",
    "The **ARIMA (AutoRegressive Integrated Moving Average)** class of models is a popular statistical technique in time series forecasting. It exploits different standard temporal structures seen in time series processes.\n",
    "\n",
    "> *Exponential smoothing and ARIMA models are the two most widely used approaches to time series forecasting, and provide complementary approaches to the problem. While exponential smoothing models are based on a description of the trend and seasonality in the data, ARIMA models aim to describe the autocorrelations in the data.* - [Source](#hyndman)\n",
    "\n",
    "##### ***Crash course on ARIMA***\n",
    "We will now take a brief look at its key features by taking apart the acronym.\n",
    "\n",
    "1. **Auto Regressive (AR)**:\n",
    "\n",
    "- Regression of a time series process onto itself (its past versions)\n",
    "- A time series process is $AR$ if its present value depends on a linear combination of past observations.\n",
    "- In financial time series, an $AR$ model attempts to explain the mean reversion and trending behaviours that we observe in asset prices.\n",
    "\n",
    "2. **Integrated (I)**:\n",
    "\n",
    "For a time series process ${X_t}$ recorded at regular intervals, the difference operation is defined as $$\\nabla X_t = X_t - X_{t-1}$$\n",
    "\n",
    "The difference operator (denoted by $\\nabla$) can be applied repeatedly. For example, \n",
    "$$\\nabla^2 X_t = \\nabla X_t - \\nabla X_{t-1}$$\n",
    "\n",
    "- A time series process is integrated of order $d$ (denoted by $I(d)$), if differencing the observations $d$ times, makes the process stationary.\n",
    "\n",
    "3. **Moving Average (MA)**:\n",
    "\n",
    "-  A time series process is $MA$ if its present value can be written as a linear combination of past error terms.\n",
    "- $MA$ models try to capture the idiosyncratic shocks observed in financial markets. We can think of events like terrorist attacks, earnings surprises, sudden political changes, etc. as the random shocks affecting the asset price movements.\n",
    "\n",
    "When we use the ARIMA class to model a time series process, each of the above components are specified in the model as parameters (with the notations $p$, $d$, and $q$ respectively). \n",
    "\n",
    "That is, the classification $ARIMA(p, d, q)$ process can be thought of as $$AR(p)I(d)MA(q)$$ \n",
    "Here,\n",
    "\n",
    "1. $p$: The number of past observations (we usually call them *lagged terms*) of the process included in the model.\n",
    "2. $d$: The number of times we difference the original process to make it stationary.\n",
    "3. $q$: The number of past error terms (we usually call them *lagged error terms* or *lagged residuals*) of the process included in the model.\n",
    "\n",
    "When we model our time series process with the ARIMA class, we implicitly assume that the underlying data generating process (and by extension the observations we record) is an ARIMA process. \n",
    "\n",
    "We should validate our assumptions (especially the implicit ones which slip under the radar) and recognize the limitations of our models. A well-known deficiency of ARIMA applications on financial time series is its failure to capture the phenomenon of volatility clustering. However, despite their inaccurate point estimates, they give rise to informative confidence intervals.\n",
    "\n",
    " **All of the below models would have good explanatory and predictive power only if the process is stationary.**\n",
    "\n",
    "$$AR(1) : x_t = \\phi x_{t-1} + \\epsilon_t$$\n",
    "$$MA(1) : x_t = \\epsilon_t + \\theta \\epsilon_{t-1}$$\n",
    "\n",
    "$$AR(p) : x_t = \\phi_1 x_{t-1} + \\phi_2 x_{t-2} + \\dots + \\phi_p x_{t-p} + \\epsilon_t$$\n",
    "$$MA(q) : x_t = \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\dots + \\theta_q \\epsilon_q$$\n",
    "\n",
    "$$ARMA(p, q) : x_t = \\phi_1 x_{t-1} + \\phi_2 x_{t-2} + \\dots + \\phi_p x_{t-p} + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\dots + \\theta_q \\epsilon_q$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregressive models\n",
    "\n",
    "Etymologically, autoregressive = auto (self) + regressive\n",
    "\n",
    "As we know, regression is the method by which we try to predict something based on other things.\n",
    "In autoregression, we try to predict something based on the past values of that same thing.\n",
    "\n",
    "The something I refer to above, could be the price of some variable we're interested in, the weight of an object, the quantity of a commodity consumed, etc.\n",
    "\n",
    "Let's start with an autoregressive model of order 1, denoted as $AR(1)$.\n",
    "\n",
    "$$AR(1): S_t = \\phi_0 + \\phi_1 S_{t-1} + \\epsilon_t$$\n",
    "\n",
    "Here,\n",
    "- $S_t$: Value of the variable $S$ at time $t$\n",
    "- $S_{t-1}$: Value of the variable $S$ at time $t-1$\n",
    "- $\\phi_0$: the intercept\n",
    "- $\\phi_1$: the parameter/slope we want to estimate\n",
    "- $\\epsilon_t$: the error/residual at time $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some parts of the material below, I directly use from the **Quantra** course on *Financial Time Series Analysis*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forecasting prices using the $AR(1)$ model\n",
    "\n",
    "We now train the AR(1) model using the ARMA/ARIMA method from the `statsmodels` library.\n",
    "\n",
    "The ARMA/ARIMA method can be imported as below\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.arima_model import ARMA, ARIMA\n",
    "```\n",
    "\n",
    "Using the `ARMA` or `ARIMA` method, the autoregressive model can be trained as\n",
    "\n",
    "```python\n",
    "ARMA(data, (p, q)\n",
    "```\n",
    "or\n",
    "```python\n",
    "\n",
    "ARIMA(data, (p, d, q))\n",
    "```\n",
    "\n",
    "where\n",
    "\n",
    "- p is the AR parameter, that needs to be defined or worked out.\n",
    "- d is the difference parameter. This will be zero in case of AR models.\n",
    "- q is the MA parameter. This will also be zero in case of an AR model.\n",
    "Hence, the autoregressive model can be trained as\n",
    "\n",
    "`ARIMA(data, (p, 0, 0))`\n",
    "or\n",
    "`ARMA(data, (p, 0))`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.style.use(\"seaborn-talk\")\n",
    "import yfinance as yf\n",
    "import nasdaqdatalink\n",
    "\n",
    "\n",
    "import scipy.stats as scs\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from statsmodels.tsa.arima_model import ARMA, ARIMA\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start1 = datetime.date(2011, 12, 25)\n",
    "end1 = datetime.date(2022, 12, 31)\n",
    "ticker1 = \"NFLX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = yf.download(ticker1, start=start1, end=end1)\n",
    "print(f\"Downloaded {df.shape[0]} rows and {df.shape[1]} columns of {ticker1} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resampling to obtain weekly stock prices with the following rules\n",
    "## 'Open': first opening price of the month\n",
    "## 'High': max price of the month\n",
    "## 'Low': min price of the month\n",
    "## 'Close' and 'Adj Close': last closing price of the month\n",
    "\n",
    "df1 = df1.resample('W').agg({'Open':'first', 'High':'max', 'Low': 'min', \n",
    "                             'Close':'last', 'Adj Close':'last'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.drop(columns=[\"Open\", \"High\", \"Low\", \"Close\"], inplace=True)\n",
    "df1.rename(columns = {'Adj Close': 'adj_close'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the series\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(df1['adj_close'], color='blue')\n",
    "plt.title(f\"{ticker1} prices over time\", fontsize=16)\n",
    "plt.xlabel(\"Years\", fontsize=12)\n",
    "plt.ylabel(\"Price in US$\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "for eachyear in range(start1.year, end1.year):\n",
    "    plt.axvline(pd.to_datetime(str(eachyear)+'-12-31'), color='black', linestyle='--', alpha=0.4)\n",
    "\n",
    "plt.axhline(df1['adj_close'].mean(), label='Mean price', color='maroon', linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.graphics.tsa.plot_acf(df1['adj_close'], lags=50, alpha=0.05);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.graphics.tsa.plot_pacf(df1['adj_close'], lags=50, alpha=0.05);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on PACF, and to keep things simple, let's start with an AR(1) model to fit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set the train and test data\n",
    "\n",
    "train_end = datetime.date(2021, 7, 2)\n",
    "test_end = end1\n",
    "\n",
    "df1_train = df1[:train_end]\n",
    "df1_test = df1[train_end + pd.Timedelta(days=1): test_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape, df1_train.shape, df1_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instantiate/define the model\n",
    "\n",
    "model1a = ARMA(df1_train['adj_close'], order=(1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit the model\n",
    "starting = time.time()\n",
    "model1a_fit = model1a.fit(disp=0)\n",
    "ending = time.time()\n",
    "print(f\"Time taken to fit the model:{round(ending-starting, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model summary\n",
    "\n",
    "print(model1a_fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted model:\n",
    "\n",
    "From the output above, we see that\n",
    "\n",
    "$\\phi_0 = 245.79$, and $\\phi_1 = 0.99$\n",
    "\n",
    "So the model works out to \\\n",
    "$\\hat{S_t} = 245.79 + 0.99 S_{t-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_start = df1_test.index[0]\n",
    "predict_end = df1_test.index[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the output in the test set\n",
    "starting = time.time()\n",
    "predicted_values = model1a_fit.predict(start= predict_start, end = predict_end)\n",
    "residual_values = df1_test['adj_close'] - predicted_values\n",
    "ending = time.time()\n",
    "\n",
    "print(f\"Time taken to forecast out-of-sample: {round(ending-starting, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the residuals\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(residual_values)\n",
    "plt.title(f\"Residuals from the AR(1) model\", fontsize=16)\n",
    "plt.xlabel(\"Years\", fontsize=12)\n",
    "plt.ylabel(\"Errors\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "\n",
    "for every3months in pd.date_range(start=predict_start, end = predict_end, freq='3M'):\n",
    "    plt.axvline(every3months, color='black', linestyle='--', alpha=0.4)\n",
    "\n",
    "\n",
    "plt.axhline(0, label='Zero error', color = 'purple', linestyle='--', alpha=0.7)\n",
    "plt.axhline(residual_values.mean(), label='Mean error', color='maroon', linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predicted v/s the actual values\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(df1_test['adj_close'])\n",
    "plt.plot(predicted_values)\n",
    "plt.title(f\"Predicted v/s actuals from the AR(1) model\", fontsize=16)\n",
    "plt.xlabel(\"Time\", fontsize=12)\n",
    "plt.ylabel(\"Prices\", fontsize=16)\n",
    "\n",
    "for every3months in pd.date_range(start=predict_start, end = predict_end, freq='3M'):\n",
    "    plt.axvline(every3months, color='black', linestyle='--', alpha=0.4)\n",
    "    \n",
    "plt.legend(('Actual', 'Predicted'), fontsize=16 )\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean absolute percent error:\", round(np.mean(abs(residual_values/df1_test['adj_close'])), 3))\n",
    "print(\"Root mean squared error:\", round(np.sqrt(np.mean(residual_values**2)), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the plots, and the errors we've encountered, the above approach is not a great way to go about it.\n",
    "\n",
    "### What's the alternative then?\n",
    "\n",
    "#### Answer: Rolling forecasts\n",
    "\n",
    "Many machine learning models are not time-indexed. The conventional approach of train-test split (80:20, 90:10, etc.) is quite a good way to build and use them for forecasts. But with time series, if we try to forecast the final 20% based on the initial 80%, we'll probably do alright for the first few predictions. However, as we make further forecasts, we rely on previous forecasts to make the next one. So the further we predict, the worse they tend to be.\n",
    "\n",
    "In order to take advantage of the temporal structure of our data, we can instead predict one or two time periods ahead. We then update our models after each time period to forecast the next one or two time periods. That is, we create rolling forecasts, which, although computationally intensive, are better than the earlier approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instantiate/define the model\n",
    "\n",
    "model1b = ARMA(df1_train['adj_close'], order=(1, 0))\n",
    "\n",
    "## Fit the model\n",
    "starting = time.time()\n",
    "model1b_fit = model1b.fit(disp=0)\n",
    "ending = time.time()\n",
    "print(f\"Time taken to fit the model: {round(ending-starting, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the price of 1 period ahead\n",
    "def predict_price_AR(train_data):\n",
    "    # Define the model\n",
    "    model = ARMA(train_data, order=(1, 0))\n",
    "\n",
    "    # Fit the model\n",
    "    model_fit = model.fit(disp=0)\n",
    "\n",
    "    # Make the forecast\n",
    "    forecast_results = model_fit.forecast()\n",
    "\n",
    "    return forecast_results[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_window = df1_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting = time.time()\n",
    "\n",
    "# Predict the price using `predict_price_AR` function\n",
    "df1['predicted_price'] = df1['adj_close'].rolling(rolling_window).apply(predict_price_AR)\n",
    "# Shift the predicted price by 1 period\n",
    "df1['predicted_price'] = df1['predicted_price'].shift(1)\n",
    "df1['error'] = df1['adj_close'] - df1['predicted_price']\n",
    "ending = time.time()\n",
    "print(f\"Time taken for the rolling forecasts: {round(ending-starting, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the residuals\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(df1['error'])\n",
    "plt.title(f\"Residuals from the AR(1) model\", fontsize=16)\n",
    "plt.xlabel(\"Years\", fontsize=12)\n",
    "plt.ylabel(\"Errors\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "\n",
    "for every3months in pd.date_range(start=predict_start, end = predict_end, freq='3M'):\n",
    "    plt.axvline(every3months, color='black', linestyle='--', alpha=0.4)\n",
    "\n",
    "\n",
    "plt.axhline(0, label='Zero error', color = 'purple', linestyle='--', alpha=0.7)\n",
    "plt.axhline(df1['error'].mean(), label='Mean error', color='maroon', linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predicted v/s the actual values\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(df1_test['adj_close'])\n",
    "plt.plot(df1['predicted_price'])\n",
    "plt.title(f\"Predicted v/s actuals from the AR(1) model\", fontsize=16)\n",
    "plt.xlabel(\"Time\", fontsize=12)\n",
    "plt.ylabel(\"Prices\", fontsize=16)\n",
    "\n",
    "for every3months in pd.date_range(start=predict_start, end = predict_end, freq='3M'):\n",
    "    plt.axvline(every3months, color='black', linestyle='--', alpha=0.4)\n",
    "    \n",
    "plt.legend(('Actual', 'Predicted'), fontsize=16 )\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean absolute percent error:\", round(np.mean(abs(df1['error']/df1.loc[predict_start:, 'adj_close'])), 3))\n",
    "print(\"Root mean squared error:\", round(np.sqrt(np.mean(df1['error']**2)), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "def model_performance(observed, predicted):\n",
    "\n",
    "    \n",
    "    \"\"\"This function will print the Mean Absolute Error, Mean Squared Error,\n",
    "    Root Mean Squared Error and Mean Absolute Percentage Error.\n",
    "    This function will also display the residula plot and the ACF.\n",
    "    \"\"\"\n",
    "\n",
    "    # Mean Absolute Error\n",
    "    mae = mean_absolute_error(observed, predicted)\n",
    "    print('The Mean Absolute Error is %.2f' % mae)\n",
    "\n",
    "    # Mean Squared Error\n",
    "    mse = mean_squared_error(observed, predicted)\n",
    "    print('The Mean Squared Error is %.2f' % mse)\n",
    "\n",
    "    # Root Mean Squared Error\n",
    "    rmse = sqrt(mean_squared_error(observed, predicted))\n",
    "    print('The Root Mean Squared Error is %.2f' % rmse)\n",
    "\n",
    "    # Mean Absolute Percentage Error\n",
    "    mape = 100 * ((observed-predicted)/observed).abs().mean()\n",
    "    print('The Mean Absolute Percentage Error is %.2f' % mape)\n",
    "\n",
    "    # Residuals\n",
    "    residuals = observed - predicted\n",
    "    labels = observed.index\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 11))\n",
    "    \n",
    "    # Plot residual\n",
    "    observed.plot(ax=ax1, color='purple')\n",
    "    predicted.plot(ax=ax1, color='green')\n",
    "    ax1.set_ylabel('Price')\n",
    "    ax1.set_title('Predicted Vs. Observed')\n",
    "    \n",
    "    # Plot residual\n",
    "    ax2.fill_between(residuals.index, residuals.values, color='red')\n",
    "    ax2.set_ylabel('Error')\n",
    "    ax2.set_xlabel('Date')\n",
    "    ax2.set_title('Residual')\n",
    "    ax2.set_xticklabels(labels, rotation=45, ha='right')\n",
    "    ax2.xaxis.set_major_formatter(mdates.DateFormatter(\"%d-%m-%Y\"))\n",
    "    ax2.xaxis.set_minor_formatter(mdates.DateFormatter(\"%d-%m-%Y\"))\n",
    "    _=plt.xticks(rotation=45)\n",
    "    \n",
    "    # Autocorrelation plot of residuals\n",
    "    sm.graphics.tsa.plot_pacf(residuals, ax=ax3, color='blue')\n",
    "    ax3.set_xlabel('Lags')\n",
    "    ax3.set_ylabel('Partial Autocorrelation')\n",
    "    ax3.set_title('Partial Autocorrelation of Residuals')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call `model_performance` to evaluate the performance of the model\n",
    "model_performance(df1['adj_close'].iloc[rolling_window:],\n",
    "                  df1['predicted_price'].iloc[rolling_window:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MA model of order q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv file using read_csv method of pandas\n",
    "data = pd.read_csv(\"wheat_etf_price_2011_2020.csv\", index_col=0)\n",
    "data.index = pd.to_datetime(data.index)\n",
    "\n",
    "# Drop the missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Rolling Window\n",
    "rolling_window = int(len(data)*0.70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='optimise'></a> \n",
    "## Optimizing the order (q) of the MA model\n",
    "\n",
    "<a id='lag-period'></a> \n",
    "## Find the lag period (q)\n",
    "Let us find the optimal lag period or order for the given time series using the ACF plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For statistical analysis\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "\n",
    "# Import and filter warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARMA',\n",
    "                        FutureWarning)\n",
    "\n",
    "# Import matplotlib and set the style\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "plot_acf(data['Adj Close'], lags=20, ax=ax)\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, you can see that the lag terms upto 10 is statistically significant. So we train the MA(10) model using `ARIMA` from the `statsmodels` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the moving average model of order 10\n",
    "model_fit_0 = ARIMA(data['Adj Close'][:rolling_window], (0, 1, 10)).fit()\n",
    "print(model_fit_0.params.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output above, you can see that the fitted model is\n",
    "$$ MA(10) = y_t = -0.23 + \\epsilon_t + 0.09*\\epsilon_{t-1} + ~...~ + 0.22*y_{t-10}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the price of 1 day\n",
    "def predict_price_MA(train_data):\n",
    "    # Define model\n",
    "    model = ARIMA(train_data, order=(0, 1, 10))\n",
    "    # Fit the model\n",
    "    model_fit = model.fit(disp=0, start_params=model_fit_0.params)\n",
    "    # Make forecast\n",
    "    forecast_results = model_fit.forecast()\n",
    "    return forecast_results[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will predict the values of the series and it is possible it will take time to run.\n",
    "starting = time.time()\n",
    "# Predict the price using `predict_price_AR` function\n",
    "data['predicted_price'] = data['Adj Close'].rolling(rolling_window).apply(\n",
    "    predict_price_MA)\n",
    "# Shift the predicted price by 1 period\n",
    "data['predicted_price'] = data['predicted_price'].shift(1)\n",
    "data.round(2).tail()\n",
    "\n",
    "ending = time.time()\n",
    "\n",
    "print(f\"Time taken to forecast prices is {ending-starting} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call `model_performance` to evaluate the performance of the model\n",
    "model_performance(data['Adj Close'].iloc[rolling_window:],\n",
    "                  data['predicted_price'].iloc[rolling_window:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the first two plots, you can see that the residuals are more positive than negative. Hence the model made lower predictions in most cases. From the third plot, you can see that there is autocorrelation between the residuals with its 3rd and 8th lagged value.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA models\n",
    "\n",
    "We now fit an ARIMA model to the weekly stock prices (from mid-2010 to mid-2019) of Netflix and learn to evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start8 = datetime.date(2010, 6, 30)\n",
    "end8 = datetime.date(2022, 7, 1)\n",
    "ticker8 = \"NFLX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = yf.download(ticker8, start=start8, end=end8, progress=False)\n",
    "print(f\"Downloaded {df.shape[0]} rows and {df.shape[1]} columns of {ticker8} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "################# PLEASE USE BELOW STATEMENTS IF NEEDED #####################\n",
    "#############################################################################\n",
    "\n",
    "## If you have don't have quandl, you can read the csv file as shown.\n",
    "\n",
    "# mydateparser = lambda x: pd.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S%z\")\n",
    "# df.to_csv(f\"{ticker8}.csv\")\n",
    "# df4 = pd.read_csv(\"NFLX.csv\", index_col=0, parse_dates=True)\n",
    "# df1 = pd.read_csv(\"NSE_5min_interval.csv\", index_col=0, parse_dates=True, date_parser=mydateparser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df8 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resampling to obtain weekly stock prices with the following rules\n",
    "## 'Open': first opening price of the month\n",
    "## 'High': max price of the month\n",
    "## 'Low': min price of the month\n",
    "## 'Close' and 'Adj Close': last closing price of the month\n",
    "\n",
    "df8 = df8.resample('W').agg({'Open':'first', 'High':'max', 'Low': 'min', \n",
    "                             'Close':'last', 'Adj Close':'last'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8.drop(columns=[\"Open\", \"High\", \"Low\", \"Close\"], inplace=True)\n",
    "df8.rename(columns = {'Adj Close': 'adj_close'}, inplace=True)\n",
    "print(df8.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking for null values\n",
    "\n",
    "df8[df8['adj_close'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start8_str = (start8 + pd.Timedelta(\"5 days\")).strftime(\"%B %Y\")\n",
    "end8_str = (end8 - pd.Timedelta(\"5 days\")).strftime(\"%B %Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.2)\n",
    "df8['adj_close'].plot(figsize=(12, 8), title=f\"{ticker8} weekly adjusted close prices ({start8_str} - {end8_str})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_stationarity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "check_stationarity(df8['adj_close'], wl1=4, wl2=52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- The p-value is nearly 1 (and equivalently the test statistic is greater than the critical values at all 3 significance levels). So the ADF test result is that the price series is non-stationary.\n",
    "- The rolling means and volatility plots are time-varying. So we arrive at the same conclusion by examining the plots.\n",
    "- From the ACF, there are significant autocorrelations above the 95% confidence interval at all lags. From the PACF, we have spikes at lags 1, 8, 9, 13, 18, 23 and 38."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8['log_returns'] = np.log(df8['adj_close'] / df8['adj_close'].shift(1))\n",
    "df8.dropna(axis='rows', how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "check_stationarity(df8['log_returns'], wl1=4, wl2=52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- As per the ADF test results, the `Netflix` returns are stationary since the p-value is almost 0 and the test statistic is less than all the critical values.\n",
    "- The returns and rolling means of the returns are all centred around 0. As the time scale increases, the means become more and more constant. At shorter time scales, the noise tends to obscure the signal.\n",
    "- The volatily is time-varying at both the faster and slower rolling levels.\n",
    "- We can see bristles near or beyond the blue shadow at lags 17 and 26 in the ACF plot and lags 12, 16, 17, 18 and 26 in the PACF plot.\n",
    "- **Returns are log price differences. So we can also infer from the above two checks, that the price series is integrated with order $1.$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "##########################################################\n",
    "################ ARIMA model fitting #####################\n",
    "##########################################################\n",
    "import scipy.stats as scs\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "\n",
    "## Creating the ARIMA model class\n",
    "## We select the order arbitrarily (p and q)\n",
    "## We inferred d from the results of `check_stationarity`\n",
    "\n",
    "## Defining the model by providing the training set and providing the parameters p, d, q\n",
    "arima_model = ARIMA(df8['adj_close'], order=(3, 1, 2))\n",
    "\n",
    "## Fitting the model, disp=0 is to switch off verbose display\n",
    "arima_fit1 = arima_model.fit(disp=0)\n",
    "\n",
    "## Printing a summary of the model\n",
    "arima_fit1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted model:\n",
    "\n",
    "$\\hat{y_t} = y_{t-1} + 0.25 - 0.29{y_{t-1}} - 0.946{y_{t-2}} + 0.055{y_{t-3}} + 0.29{\\epsilon_{t-1}} + 0.29{\\epsilon_{t-1}}$\n",
    "\n",
    "\n",
    "\n",
    "Points to note:\n",
    "- We chose an $ARIMA(3, 1, 2)$ model to fit the price series of `Netflix`. Equivalently, we could have fit an $ARIMA(3, 0, 2)$ to the returns instead. \n",
    "- The `summary()` method provides the results of the model fitting exercise on the **in-sample data set** (a.k.a. the training data).\n",
    "- The most important part is the table at the centre which has the coefficient values, their 95% confidence intervals and their corresponding p-values.\n",
    "- However, we also need to run model diagnostics by examining the residual errors closely. This will tell us if our model was a good fit to the underlying data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arima_diagnostics(resids_, figsize=(15, 9), n_lags=40):\n",
    "    '''\n",
    "    Diagnoses the fit of an ARIMA model by examining its residuals.\n",
    "    Returns a chart with with multiple plots\n",
    "    '''\n",
    "    # Creating placeholder subplots\n",
    "    M = 2\n",
    "    N = 2\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(M, N, figsize=figsize)\n",
    "\n",
    "    r = resids_\n",
    "    resids_ = (r - np.nanmean(r)) / np.nanstd(r)\n",
    "    resids_nonmissing = resids_[~(np.isnan(resids_))]\n",
    "    \n",
    "    # Plotting residuals over time\n",
    "    sns.lineplot(x=np.arange(len(resids_)), \n",
    "                 y=resids_, ax=ax1)\n",
    "    ax1.set_title('Standardized residuals')\n",
    "\n",
    "    # Plotting the distribution of residuals\n",
    "    x_lim = (-1.96 * 2, 1.96 * 2)\n",
    "    r_range = np.linspace(x_lim[0], x_lim[1])\n",
    "    norm_pdf = scs.norm.pdf(r_range)\n",
    "    \n",
    "    sns.distplot(resids_nonmissing, hist=True, kde=True, \n",
    "                 norm_hist=True, ax=ax2)\n",
    "    ax2.plot(r_range, norm_pdf, color='green', linewidth=2, label='N(0,1)')\n",
    "    ax2.set_title('Distribution of standardized residuals')\n",
    "    ax2.set_xlim(x_lim)\n",
    "    ax2.legend()\n",
    "        \n",
    "    # Q-Q plot\n",
    "    qq = sm.qqplot(resids_nonmissing, line='s', ax=ax3)\n",
    "    ## 's' is for standardized line to compare the plot with a normal distribution\n",
    "    ax3.set_title('Q-Q plot')\n",
    "\n",
    "    # ACF plot\n",
    "    sm.graphics.tsa.plot_acf(resids_, lags=n_lags, ax=ax4, alpha=0.05)\n",
    "    ax4.set_title('ACF plot')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sns.set(font_scale=1.2)\n",
    "arima_diagnostics(arima_fit1.resid)\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "    \n",
    "- `Standardized residuals`: The mean of the residuals is approximately zero. However, it's variance is much higher in the second half of the series.\n",
    "- `Distribution of standardized residuals` and `Q-Q plot`: Both plots indicate fatter tails compared to a normal distribution.\n",
    "- `ACF plot`: There seems to be serial correlations at lags 8, 13, 14, 22 and a few more. \n",
    "- **If the fit is good, we should see residuals similar to Gaussian white noise. It's not so here.**\n",
    "- So we can infer that the model is not a very good fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Statistical tests we additionally run\n",
    "\n",
    "1. *To check for autocorrelations in residuals: [`The Ljung-Box test`](https://en.wikipedia.org/wiki/Ljung%E2%80%93Box_test)*\n",
    "\n",
    "The null hypothesis is that the serial correlations of the time series are zero. We use it in addition to visual interpretation of ACF/PACF plots.\n",
    "\n",
    "2. *To check for normality in residuals: [`The Jarque-Bera test`](https://en.wikipedia.org/wiki/Jarque-Bera_test)*\n",
    "\n",
    "The null hypothesis is that the time series is normally distributed. We use it in addition to visual interpretation of plots like the residual distribution and the Q-Q plots.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Running the Ljung-Box test and plotting the results\n",
    "\n",
    "ljung_box_results = sm.stats.acorr_ljungbox(arima_fit1.resid)\n",
    "fig, ax = plt.subplots(1, figsize=(10, 6))\n",
    "sns.scatterplot(x=range(len(ljung_box_results[1])), y=ljung_box_results[1], ax=ax)\n",
    "ax.axhline(0.05, ls='--', color='red')\n",
    "ax.set(title=f\"Ljung-Box test results (after modeling {ticker8} stock prices)\", xlabel='Lags', ylabel='p-value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "    \n",
    "- There are no significant serial correlations until lag 12.\n",
    "- However, many of the correlations from lag 13 are below the red line.\n",
    "- So our model is not a good fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Running the Jarque-Bera test and interpreting its results\n",
    "\n",
    "from statsmodels.stats.stattools import jarque_bera\n",
    "\n",
    "jb_test_stat, pvalue, _, _ = jarque_bera(arima_fit1.resid)\n",
    "print(f\"Jarque-Bera statistic: {jb_test_stat:.2f} with p-value: {pvalue:.2f}\")\n",
    "\n",
    "if pvalue < 0.05:\n",
    "    print(\"Our residuals are likely not normally distributed.\")\n",
    "else:\n",
    "    print(\"Our residuals are likely normally distributed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ***********************************************************\n",
    "## ******* Manually checking for best ARIMA ******************\n",
    "## ***********************************************************\n",
    "\n",
    "############## Please try on your own time ##################\n",
    "## Make necessary modifications (if needed) to the below code\n",
    "\n",
    "\n",
    "# %%time\n",
    "\n",
    "# best_aic = np.inf\n",
    "# best_order = None\n",
    "# best_mdl = None\n",
    "\n",
    "# pq_rng = range(5)\n",
    "# d_rng = range(2)\n",
    "# for i in pq_rng:\n",
    "#     for d in d_rng:\n",
    "#         for j in pq_rng:\n",
    "#             try:\n",
    "#                 tmp_mdl = ARIMA(df8['adj_close'], order=(i, d, j)).fit(method='mle', trend='nc')\n",
    "#                 tmp_aic = tmp_mdl.aic\n",
    "#                 if tmp_aic < best_aic:\n",
    "#                     best_aic = tmp_aic\n",
    "#                     best_order = (i, d, j)\n",
    "#                     best_mdl = tmp_mdl\n",
    "#             except:\n",
    "#                 continue\n",
    "            \n",
    "# print(\"aic : \",best_aic, \"| order : \",best_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Automatically finding the best ARIMA fit (using the [`pmdarima`](https://alkaline-ml.com/pmdarima/develop/about.html) library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import pmdarima as pm\n",
    "\n",
    "## Fitting the model (This is the default setting)\n",
    "arima_fit2 = pm.auto_arima(df8['adj_close'], error_action='ignore', \n",
    "                           suppress_warnings=True, seasonal=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Printing a summary of the model\n",
    "arima_fit2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## Fitting the model(With more tuning of the parameters)\n",
    "arima_fit3 = pm.auto_arima(df8['adj_close'], error_action='ignore', \n",
    "                           suppress_warnings=True, stepwise=False, \n",
    "                           approximation=False, seasonal=False)\n",
    "\n",
    "arima_fit3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "\n",
    "- The most suitable model is $ARIMA(2, 1, 2)$\n",
    "- Our guiding principle when we build models is [Occam's Razor](http://pespmc1.vub.ac.be/OCCAMRAZ.html) i.e. we want a model with the fewest parameters that can explain our time series process. \n",
    "- We therefore use information criterion ([Akaike Information Criterion](https://en.wikipedia.org/wiki/Akaike_information_criterion)).\n",
    "- When choosing from multiple competing models, we choose the one which has the smallest AIC.\n",
    "- The idea is to find the right balance between underfitting and overfitting. AIC helps us find that.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forecasting using the ARIMA class\n",
    "\n",
    "We will forecast using $ARIMA(2, 1, 2)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start9 = datetime.date(2019, 6, 30)\n",
    "end9 = datetime.date(2020, 7, 7)\n",
    "ticker9 = ticker8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = yf.download(ticker9, start=start9, end=end9, progress=False)\n",
    "print(f\"Downloaded {df.shape[0]} rows and {df.shape[1]} columns of {ticker9} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "################# PLEASE USE BELOW STATEMENTS IF NEEDED #####################\n",
    "#############################################################################\n",
    "\n",
    "## If you have don't have quandl, you can read the csv file as shown.\n",
    "\n",
    "# mydateparser = lambda x: pd.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S%z\")\n",
    "# df.to_csv(f\"{ticker8}2.csv\")\n",
    "# df4 = pd.read_csv(\"NFLX.csv\", index_col=0, parse_dates=True)\n",
    "# df1 = pd.read_csv(\"NSE_5min_interval.csv\", index_col=0, parse_dates=True, date_parser=mydateparser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resampling to obtain weekly stock prices with the following rules\n",
    "## 'Open': first opening price of the month\n",
    "## 'High': max price of the month\n",
    "## 'Low': min price of the month\n",
    "## 'Close' and 'Adj Close': last closing price of the month\n",
    "\n",
    "df9 = df9.resample('W').agg({'Open':'first', 'High':'max', 'Low': 'min', \n",
    "                             'Close':'last', 'Adj Close':'last'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9.drop(columns=[\"Open\", \"High\", \"Low\", \"Close\"], inplace=True)\n",
    "df9.rename(columns = {'Adj Close': 'adj_close'}, inplace=True)\n",
    "print(df9.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking for null values\n",
    "\n",
    "df9[df9['adj_close'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fcast3 = len(df9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Forecasting using the second model, ARIMA(2, 1, 2)\n",
    "\n",
    "arima_fcast3 = arima_fit3.predict(n_periods=n_fcast3, \n",
    "                                  return_conf_int=True, alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "arima_fcast3 = [pd.DataFrame(arima_fcast3[0], columns=['prediction']), \n",
    "                pd.DataFrame(arima_fcast3[1], columns=['lower_95', \n",
    "                                                       'upper_95'])]\n",
    "\n",
    "arima_fcast3 = pd.concat(arima_fcast3, axis=1).set_index(df9.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## Plotting the results for both models\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "\n",
    "ax = sns.lineplot(data=df9['adj_close'], color='black', label='Actual')\n",
    "\n",
    "ax.plot(arima_fcast3.prediction, color='red', label='ARIMA(2, 1, 2)')\n",
    "\n",
    "ax.fill_between(arima_fcast3.index, arima_fcast3.lower_95, \n",
    "                arima_fcast3.upper_95, alpha=0.2, \n",
    "                facecolor='red')\n",
    "\n",
    "ax.set(title=f\"{ticker8} stock price - actual vs. predicted\", xlabel='Date',\n",
    "       ylabel='Adjusted close price (US$)')\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('images/ch3_im25.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A brief look at modeling volatility using the ARCH/GARCH family of models\n",
    "\n",
    "The ARIMA class of models is widely used in asset price forecasting. However, as we have seen, time-varying volatility and volatility clustering (heteroskedasticity) are recurrent themes in finance. The ARMA/ARIMA models do not account for it. Fortunately, we have the ARCH/GARCH method which allows us to model for the time-dependent change in the volatility of a time-series.\n",
    "\n",
    "\n",
    "We model conditional variance in Python with the `arch` library.\n",
    "\n",
    "I quote material and code snippets from the [Quantra course on time series](https://quantra.quantinsti.com/course/financial-time-series-analysis-trading) for the rest of the material below.\n",
    "\n",
    "\n",
    "## Autoregressive Conditionally Heteroskedastic Models (ARCH)\n",
    "\n",
    "If we observe the name of the model, we can make a pretty good guess what it does. It estimates the conditional variance($\\sigma_t$ in the below setup) over time based on the past values of the variance (hence the name autoregressive). We now consider the $ARCH(1)$ model below which can be easily generalized to $ARCH(q)$.\n",
    "\n",
    "$$r_t = \\mu + \\epsilon_t$$ \n",
    "\n",
    "Above, we express returns as a (multiplicative) combination of deterministic and stochastic components.\n",
    "$$\\epsilon_t = \\sigma_t w_t$$ Carrying on from the previous step, we express the stochastic component as a combination of Gaussian white noise $w_t$ and the conditional standard deviation $\\sigma_t$.\n",
    "<div class=\"alert alert-success\">$$\\sigma_{t+1}^2 = \\alpha_0 + \\alpha_1 \\epsilon_{t}^2$$</div> \n",
    "\n",
    "where \n",
    "* $\\sigma_{t+1} ^ 2$ is the ARCH predicted variance,\n",
    "* $\\epsilon_{t} ^ 2$ is the current period residual,\n",
    "* $\\alpha_0$ is a constant and\n",
    "* $\\alpha_1$ is the autoregressive coefficient.\n",
    "\n",
    "\n",
    "***This is the ARCH model where $\\alpha_0$ and $\\alpha_1$ are its parameters.***\n",
    "\n",
    "It is useful to think of $ARCH(q)$ as the application of $AR(p)$ to the variance of a time series process.\n",
    "\n",
    "The ARCH model in Python is implemented using the `arch_model` function. The syntax is shown below.\n",
    "\n",
    "Syntax: \n",
    "```python\n",
    "arch_model(historical_data, vol='ARCH', p=order_of_ARCH_model, dist=distribution_of_the_data)\n",
    "```\n",
    "1. historical_data: Historical data whose volatility is to be predicted\n",
    "1. vol='ARCH': Type of model to use\n",
    "1. order_of_ARCH_model: The order of the ARCH(p) model\n",
    "1. distribution_of_the_data: distribution of the input data. Empirically, for most financial data, this is Skewed Studentâ€™s-t distribution or 'skewt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start10 = datetime.date(2020, 1, 1)\n",
    "end10 = datetime.date(2022, 9, 25)\n",
    "ticker10 = \"HINDUNILVR.NS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = yf.download(ticker10, start=start10, end=end10, progress=False)\n",
    "print(f\"Downloaded {df.shape[0]} rows and {df.shape[1]} columns of {ticker10} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10.drop(columns=[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"], inplace=True)\n",
    "df10.rename(columns = {'Adj Close': 'adj_close'}, inplace=True)\n",
    "print(df10.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10['simple_perc_returns'] = 100 * df10['adj_close'].pct_change().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10['simple_perc_returns'].plot(title=f'Daily percentage returns of {ticker10}: {start10} to {end10}', figsize=(12, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annualized volatilty\n",
    "df10['actual_historical_volatility'] = df10['simple_perc_returns'].rolling(14).std() * ((252)**0.5)\n",
    "\n",
    "# Print the last 5 rows\n",
    "df10.round(2).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10['actual_historical_volatility'].plot(figsize=(10, 7))\n",
    "# Set title and labels for the plot\n",
    "plt.title('Annualized historical volatility of the returns', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Historical volatility (%)', fontsize=10)\n",
    "plt.legend(['Hindustan Unilever historical volatility'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arch\n",
    "import time\n",
    "\n",
    "\n",
    "# Function to predict the volatility of 1 day\n",
    "def predict_volatility(hist_returns_data):\n",
    "    # Define model\n",
    "    gm = arch.arch_model(hist_returns_data, vol='ARCH', p=1, o=0, q=0, dist='skewt')\n",
    "\n",
    "    # Fit the model\n",
    "    gm_fit = gm.fit(disp='off') \n",
    "    \n",
    "    # Make forecast\n",
    "    forecast_variance = gm_fit.forecast(horizon=1).variance.values[-1]\n",
    "\n",
    "    # Forecast volatility\n",
    "    forecast_volatility = forecast_variance**0.5\n",
    "\n",
    "    # Calculate and return the annualized forecast variance\n",
    "    annualized_volatility = forecast_volatility * (252**0.5)\n",
    "    return annualized_volatility\n",
    "\n",
    "start_time = time.time()\n",
    "# Calculate the ARCH predicted volatility for the each day    \n",
    "df10['ARCH_predicted_volatility'] = df10['simple_perc_returns'].rolling(252).apply(predict_volatility, raw=True)\n",
    "\n",
    "# Shift the ARCH predicted volatility to match with the actual historical volatility on each day    \n",
    "df10['ARCH_predicted_volatility'] = df10['ARCH_predicted_volatility'].shift(1)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken to forecast volatility using ARCH = {np.round(end_time - start_time)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing an example of the normal market scenario\n",
    "df10.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Autoregressive Conditionally Heteroskedastic Models (GARCH)\n",
    "\n",
    "\n",
    "The GARCH model is the generalized ARCH model and is implemented in the same fashion as the ARCH model. The GARCH model is then used to predict volatility. \n",
    "\n",
    "The equation of the GARCH model is:\n",
    "\n",
    "$$\n",
    "\\sigma_{t+1} ^ 2 = \\alpha_0 + \\alpha_1 * r_t ^2 + \\beta_1 * \\sigma_{t} ^ 2\n",
    "$$\n",
    "where \n",
    "* $\\sigma_{t+1} ^ 2$ is the ARCH predicted variance,\n",
    "* $r_{t} ^ 2$ is the current period returns,\n",
    "* $\\alpha_0$ is a constant,\n",
    "* $\\alpha_1$ is the autoregressive coefficient and\n",
    "* $\\beta_1$ is the conditional regression coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the volatility of 1 day\n",
    "def predict_volatility_garch(hist_returns_data):\n",
    "    # Define model\n",
    "    gm = arch.arch_model(hist_returns_data, vol='GARCH', p=1, q=1, dist='skewt')\n",
    "\n",
    "    # Fit the model\n",
    "    gm_fit = gm.fit(disp='off') \n",
    "    \n",
    "    # Make forecast\n",
    "    forecast_variance = gm_fit.forecast(horizon=1).variance.values[-1]\n",
    "\n",
    "    # Forecasted volatility\n",
    "    forecast_volatility = forecast_variance**0.5\n",
    "\n",
    "    # Calculate and return the annualised forecasted variance\n",
    "    annualized_volatility = forecast_volatility * (252**0.5)\n",
    "    return annualized_volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Calculate the GARCH predicted volatility for the each day    \n",
    "df10['GARCH_predicted_volatility'] = df10['simple_perc_returns'].rolling(252).apply(predict_volatility_garch, raw=True)\n",
    "\n",
    "# Shift the GARCH predicted volatility to match with the actual historical volatility on each day    \n",
    "df10['GARCH_predicted_volatility'] = df10['GARCH_predicted_volatility'].shift(1)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken to forecast volatility using GARCH = {np.round(end_time - start_time)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10['actual_historical_volatility'].plot(figsize=(10, 7))\n",
    "df10['GARCH_predicted_volatility'].plot(figsize=(10, 7), color='green')\n",
    "# Set title and labels for the plot\n",
    "plt.title('GARCH Predicted Volatility and Historical Volatility', fontsize=14)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Volatility (%)', fontsize=12)\n",
    "plt.legend(['Historical Volatility', 'GARCH predicted volatility'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10['actual_historical_volatility'].plot(figsize=(10, 7))\n",
    "df10['ARCH_predicted_volatility'].plot(figsize=(10, 7), color='red')\n",
    "# Set title and labels for the plot\n",
    "plt.title('ARCH Predicted Volatility and Historical Volatility', fontsize=14)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Volatility (%)', fontsize=12)\n",
    "plt.legend(['Historical Volatility', 'ARCH predicted volatility'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'references'></a>\n",
    "#### References\n",
    "<a id = 'bnshephard'></a>\n",
    "<a id = 'arch'></a>\n",
    "<a id = 'others'></a>\n",
    "<a id = 'eryk'></a>\n",
    "<a id = 'cont2001'></a>\n",
    "<a id = 'hyndman'></a>\n",
    "\n",
    "1. Brownlee, Jason. Introduction to time series forecasting with python: how to prepare data and develop models to predict the future. Machine Learning Mastery (2018).\n",
    "2. Cochrane, John H. \"Time series for macroeconomics and finance.\" Manuscript, University of Chicago (2005).\n",
    "3. Cont, R. Empirical properties of asset returns: stylized facts and statistical issues (2001).\n",
    "4. https://towardsdatascience.com/@eryk.lewinson\n",
    "5. https://pyflux.readthedocs.io/en/latest/getting_started.html\n",
    "6. https://tomaugspurger.github.io/modern-7-timeseries\n",
    "7. https://arch.readthedocs.io/en/latest/univariate/univariate_volatility_modeling.html\n",
    "8. https://www.statsmodels.org/devel/examples/notebooks/generated/exponential_smoothing.html\n",
    "9. Hyndman, R.J., & Athanasopoulos, G. Forecasting: principles and practice, 2nd edition, OTexts: Melbourne, Australia. OTexts.com/fpp2. Accessed on 11 July 2020.\n",
    "6. Tsay, Ruey S. Analysis of financial time series. Vol. 543. John Wiley & Sons (2005).\n",
    "7. Campbell, John Y., Andrew Wen-Chuan Lo, and Craig MacKinlay. The Econometrics of Financial Markets. Vol. 2. Princeton, NJ: princeton University press (1997).\n",
    "10. http://www.blackarbs.com/blog/time-series-analysis-in-python-linear-models-to-garch/11/1/2016\n",
    "11. https://quantra.quantinsti.com/course/financial-time-series-analysis-trading\n",
    "12. https://github.com/anejad/Exploring-Market-Seasonality-Trends\n",
    "13. https://quantra.quantinsti.com/course/financial-time-series-analysis-trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
